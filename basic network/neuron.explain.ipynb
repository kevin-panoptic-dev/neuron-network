{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Overview\n",
    "\n",
    "In the field of machine learning, we assume that there are some connection between the input data and the output labels.\n",
    "\n",
    "For example, the stock price of Google may be related to its performance, the ability of its CEO, global market, and its competitor, etc.\n",
    "\n",
    "This can be modeled as a function:\n",
    "\n",
    "$$ stock \\ price \\ of \\ Google =  \\\\ F(performance, CEO, global market, competitor, ...) $$\n",
    "\n",
    "If we somehow figure out the connection between performance, CEO ... and the stock price, we all becomes billionaires, since we all know what's tomorrow's stock price.\n",
    "\n",
    "This naturally leads to two questions:\n",
    "\n",
    "1. *How can we find the right input that is related to the output (stock price)?*\n",
    "1. *How can we write the function in such a way that can represents this connection, instead of being a random number generator?*\n",
    "\n",
    "For the first question, we really need to think deeper about what type of input data we are using. *For example, predicting stock price using previous stock price would be a **BAD** idea*, since the thing that makes Google so valuable is not its stock price. A bad decision of the CEO can lead to stock price decrease, but neither increase or decrease in previous stock price is related to tomorrow's stock price.\n",
    "\n",
    "For the second questions, that's where the neuron network comes in. Since it's way to complicated for any human to measure the exact connections, we invent neuron network to do this part: we want the neuron network to find the connection by itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A single neuron\n",
    "\n",
    "A single neuron is just a function! It receives some inputs, and produce some outputs.\n",
    "$$ y = f(x) $$\n",
    "But as we see, there may be multiple factors that affects the output. So our neuron maybe need to take multiple inputs.\n",
    "$$ y = f(x_0, x_1, x_2, \\cdots, x_n) $$\n",
    "This makes the function unnecessary long, and instead of thinking these parameters as separate inputs, we can think them as a group of inputs.\n",
    "We represent them as a vector, which is just an array of numbers.\n",
    "$$ \\vec{x} = \\begin{bmatrix} x_0 \\\\ x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix} $$\n",
    "In python, we can represent them as a list.\n",
    "```python\n",
    "inputs = [x0, x1, x2, ..., xn]\n",
    "```\n",
    "As we mentioned before, we assume there are some connections between the inputs and the output. So we need another vector, or list, to store these connections. We call they as 'weights'.\n",
    "```python\n",
    "weights = [w0, w1, w2, ..., wn]\n",
    "```\n",
    "Since the weights are just factor that determines the influence of a certain input, we can multiply each of them with the corresponding inputs. And we just add all the influences together.\n",
    "$$ output = w_0 \\cdot x_0 + w_1 \\cdot x_1 + \\cdots + w_n \\cdot x_n = \\sum_{i=0}^{n} w_i \\cdot x_i $$\n",
    "Mathematically, we can represent this as a dot product of two vectors.\n",
    "$$ output = \\vec{w} \\cdot \\vec{x}$$\n",
    "There may also be some constant factors that affects the output (y = mx + b?). We call them 'biases'.\n",
    "$$ output = \\vec{w} \\cdot \\vec{x} + b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we as human does not know the connection...\n",
    "# we just random initialize them and hope these artificial neurons can find them.\n",
    "import random\n",
    "\n",
    "random.seed(0) # set the seed for reproducibility\n",
    "\n",
    "inputs = [1, 2, 3, 4, 5] # input data are known\n",
    "weights = [random.random() for _ in range(len(inputs))]\n",
    "bias = random.random() # also random biases\n",
    "\n",
    "output = sum([x * w for x, w in zip(inputs, weights)]) + bias\n",
    "print(output) # 7.619020145363499"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we already see the weakness: 5 random weights and 1 biases are probably not enough for representing the connection. We also want to collect more inputs instead of only 5.\n",
    "Let's start with input data. Holding all the data in a single vector would be confusing, so we categorize them based on where they from.\n",
    "```python\n",
    "data_from_source_1 = [1, 2, 3, 4, 5]\n",
    "data_from_source_2 = [6, 7, 8, 9, 10]\n",
    "data_from_source_3 = [11, 12, 13, 14, 15]\n",
    "```\n",
    "But we still only want a single input instead of '$x_0, x_1, ...$', so we grab all the data vectors and put them in another vector.\n",
    "```python\n",
    "all_inputs = [\n",
    "    data_from_source_1, \n",
    "    data_from_source_2, \n",
    "    data_from_source_3,\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape of Matrix\n",
    "\n",
    "This is call a matrix. It's a list of list, or a two dimensional list.\n",
    "All the list inside its parent list must be *homogeneous*, meaning they must have the same length.\n",
    "We delineate the size of the matrix using shape.  \n",
    "For example, *all_inputs* would have a shape of (3, 5), since there are three element in the first level and five in the second.  \n",
    "Here's the mathematical representation of the matrix:\n",
    "$$\n",
    "all\\_inputs = \\begin{bmatrix}\n",
    "1 & 2 & 3 & 4 & 5 \\\\\n",
    "6 & 7 & 8 & 9 & 10 \\\\\n",
    "11 & 12 & 13 & 14 & 15 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "And this matrix has 3 rows and 5 columns. Working with rows and columns could be tricky, so we use the term *axis* to refer to them.\n",
    "axis 0 refers to the first level, axis 1 refers to the second level...  \n",
    "Some may argue that working with axis would be even more confusing, and this is perfectly true. So we will try to avoid these terms, and just use our *shape*.\n",
    "```python\n",
    "shape[0] = len(matrix)\n",
    "shape[1] = len(matrix[0])\n",
    "```\n",
    "Assume the matrix is valid (homogeneous)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "class Matrix:\n",
    "    def __init__(self, vector_of_vectors: list):\n",
    "        self.matrix = vector_of_vectors\n",
    "    \n",
    "    def __str__(self):\n",
    "        return pprint.pformat(self.matrix)        \n",
    "\n",
    "    @property\n",
    "    def shape(self) -> tuple[int, ...]:\n",
    "        shape_list = []\n",
    "        next_layer = self.matrix\n",
    "        while isinstance(next_layer, list):\n",
    "            if len(next_layer) == 0:\n",
    "                break\n",
    "            shape_list.append(len(next_layer))\n",
    "            next_layer = next_layer[0]\n",
    "        return tuple(shape_list)\n",
    "            \n",
    "matrix = Matrix([\n",
    "    [1, 2, 3, 4],\n",
    "    [4, 5, 6, 7],\n",
    "    [7, 8, 9, 10],\n",
    "])\n",
    "assert matrix.shape == (3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape of Vector\n",
    "What about the shape of vector? Since vectors has only one dimension, we can just use a tuple with one element to represent the shape.\n",
    "```python\n",
    "def shape(vector) -> tuple[int]:\n",
    "    return (len(vector),)\n",
    "```\n",
    "And the output is just simple like this:\n",
    "```python\n",
    ">>> shape([1, 2, 3, 4, 5])\n",
    "(5,)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the input, it's time for our first neuron to take the input the produce some output.  \n",
    "We now have three groups of inputs, for each group we just calculate the dot product between these the group and the weights, then add the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = Matrix([\n",
    "    [1, 2, 3, 4],\n",
    "    [4, 5, 6, 7],\n",
    "    [7, 8, 9, 10],\n",
    "])\n",
    "weights = [random.random() for _ in range(matrix.shape[1])]\n",
    "bias = random.random()\n",
    "output = []\n",
    "for group in matrix.matrix:\n",
    "    output.append(sum([x * w for x, w in zip(group, weights)]) + bias)\n",
    "print(output) # [5.16898712243865, 12.014580879206125, 18.860174635973603]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several improvements we can give to this implementation:\n",
    "1. We can use a simpler list comprehension instead of the for loop to compute outputs.\n",
    "1. We may also want a class of vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Union\n",
    "\n",
    "class Vector:\n",
    "    def __init__(self, vector: list):\n",
    "        self.vector = vector\n",
    "    \n",
    "    def __str__(self):\n",
    "        return pprint.pformat(self.vector)        \n",
    "\n",
    "    @property\n",
    "    def shape(self) -> tuple[int]:\n",
    "        return (len(self.vector),)\n",
    "    \n",
    "    # element wise multiplication\n",
    "    def __mul__(self, other: 'Vector'):\n",
    "        return sum(x * w for x, w in zip(self.vector, other.vector))\n",
    "    \n",
    "    __rmul__ = __mul__\n",
    "\n",
    "    # add a function for dot product\n",
    "    # it's probably a bad idea to have dot function associated with 'Vector', but we will leave it here...\n",
    "    # We add will add full implementation in the later python scripts\n",
    "    def dot(self, other: Union['Matrix', 'Vector'], bias: float):\n",
    "        if isinstance(other, Matrix):\n",
    "            return Vector([sum([x * w for x, w in zip(group, self.vector)]) + bias for group in other.matrix])\n",
    "        elif isinstance(other, Vector):\n",
    "            return self * other + bias\n",
    "        else:\n",
    "            raise TypeError(f\"Expected Matrix or Vector, got {type(other).__name__}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Neuron with Vector input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Vector([1, 2, 3, 4, 5]) # input data are known\n",
    "weights = Vector([random.random() for _ in range(inputs.shape[0])])\n",
    "bias = random.random() # also random biases\n",
    "\n",
    "output = inputs * weights + bias\n",
    "print(output) # 7.619020145363499"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Neuron with Matrix input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Matrix([\n",
    "    [1, 2, 3, 4],\n",
    "    [4, 5, 6, 7],\n",
    "    [7, 8, 9, 10],\n",
    "])\n",
    "weights = Vector([random.random() for _ in range(inputs.shape[1])])\n",
    "bias = random.random()\n",
    "output = weights.dot(inputs, bias)\n",
    "print(output) # [5.16898712243865, 12.014580879206125, 18.860174635973603]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still things we need to consider next. A single neuron is still not enough to calculate the convoluted connection between the input and the output, but before we dive into the layer of neurons, we need to first clarify some concepts about shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape\n",
    "\n",
    "As we know, each vector inside the Matrix must have the same length, or we will get an shape error like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "--------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "Cell In[7], line 2\n",
    "      1 import numpy as np\n",
    "----> 2 np.array([\n",
    "      3     [1, 2, 3, 4, 5],\n",
    "      4     [6, 7, 7, 9] # incorrect shape, ERROR!\n",
    "      5 ])\n",
    "\n",
    "ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to make sure that the length of the weights vector is the same as the length of the vectors inside the input matrix.\n",
    "Think about the operation we perform:\n",
    "$$\n",
    "output = \\begin{bmatrix}\n",
    "    1 & 2 & 3 & 4 \\\\\n",
    "    4 & 5 & 6 & 7 \\\\\n",
    "    7 & 8 & 9 & 10 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    w_1 \\\\\n",
    "    w_2 \\\\\n",
    "    w_3 \\\\\n",
    "    w_4 \\\\\n",
    "\\end{bmatrix}\n",
    "+ b\n",
    "$$\n",
    "Which is equals to:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    (1 \\cdot w_1 + 2 \\cdot w_2 + 3 \\cdot w_3 + 4 \\cdot w_4) + b \\\\\n",
    "    (4 \\cdot w_1 + 5 \\cdot w_2 + 6 \\cdot w_3 + 7 \\cdot w_4) + b \\\\\n",
    "    (7 \\cdot w_1 + 8 \\cdot w_2 + 9 \\cdot w_3 + 10 \\cdot w_4) + b\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "But what if we one less weight?\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    w_1 \\\\\n",
    "    w_2 \\\\\n",
    "    w_3 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Then we have nothing to multiple with 4, 7, and 10!\n",
    "\n",
    "That's why the length of the weights vector must be the same as the length of the vectors inside the input matrix, or the second value of the shape of the input matrix.\n",
    "\n",
    "We also find that the length of the output vector is the same as the length of the input matrix, or the total number of vectors inside the input matrix."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
